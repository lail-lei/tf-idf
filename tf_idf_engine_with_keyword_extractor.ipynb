{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf-idf-keyword-extractor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPmpX10o6y9THRhVj4UWZIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lail-lei/tf-idf/blob/main/tf_idf_engine_with_keyword_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCD3rU4QETnc"
      },
      "source": [
        "# first, import our libraries \n",
        "import pandas as pd\n",
        "\n",
        "# need for importing text files from github\n",
        "import requests\n",
        "\n",
        "# Using regex\n",
        "import re\n",
        "\n",
        "# for unique id\n",
        "import uuid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aivW8n7G8sY"
      },
      "source": [
        "# helper function to process source document before indexing \n",
        "def tokenize (src):\n",
        "  #simple tokenizer\n",
        "  return re.sub(r'[^\\w\\s]', '', src.lower()).split()\n",
        "\n",
        "class document ():\n",
        "\n",
        "  document = None\n",
        "  tokenized_document = None\n",
        "  inverted_index = None # holds number of time word appears in document \n",
        "  term_frequency_index = None # holds tf of words in document (number of time word appears / total number of words)\n",
        "  _id = None\n",
        "\n",
        "  # helper function to create inverted index\n",
        "  def invert (self):\n",
        "    index = {}\n",
        "    # populate dictionary with frequency of specific document's words\n",
        "    # this is O(|D|) where D is words in a document  \n",
        "    for word in self.tokenized_document:\n",
        "      frequency = index.get(word, 0)\n",
        "      index[word] = frequency + 1\n",
        "    return index\n",
        "\n",
        "  # helper function to create term frequency index\n",
        "  def tf (self):\n",
        "    index = {}\n",
        "    totalNumberOfWords = len(self.tokenized_document)\n",
        "    # this is O(|D|) where D is words in a document \n",
        "    for word, count in self.inverted_index.items():\n",
        "      index[word] = count / float(totalNumberOfWords)\n",
        "    self.term_frequency_index = index\n",
        "    \n",
        "  # constructor\n",
        "  def __init__(self, document):\n",
        "    self._id = uuid.uuid4()\n",
        "    self.document = document\n",
        "    self.tokenized_document = tokenize(document)\n",
        "    self.inverted_index = self.invert() # holds term frequency in document \n",
        "    self.tf()\n",
        "  \n",
        "\n",
        "class searchEngine ():\n",
        "\n",
        "  # a list of all documents in search engine \n",
        "  documets = None\n",
        "\n",
        "  # an index containing the log of the number of documents \n",
        "  # divided by the number of documents that contain each specific word\n",
        "  inverse_document_frequency_index = None\n",
        "\n",
        "  # a data frame containing the TF-IDF scores for all the words in the engine\n",
        "  data_frame = None\n",
        "\n",
        "  # constructor\n",
        "  def __init__(self):\n",
        "    self.documents = []\n",
        "\n",
        "  # compute the idf score across all documents in the corpus \n",
        "  def computeIDF(self):\n",
        "    import math\n",
        "    N = len(self.documents)\n",
        "    index = {}\n",
        "    # store the frequencies of all words across all documents (global frequency)\n",
        "    # this is O(N*|D|) where N is number of documents in a corpus and D is the words in a document \n",
        "    for document in self.documents:\n",
        "        for word, frequency in document.term_frequency_index.items():\n",
        "            global_frequency = index.get(word, 0) # return 0 if doesn't exist in index\n",
        "            # increment count \n",
        "            index[word] = global_frequency + 1\n",
        "    # this is O(|T|)where T is all words in the corpus\n",
        "    for word, global_frequency in index.items():\n",
        "        # perform log of  (#documents/over the global frequency)\n",
        "        index[word] = math.log(N / float(global_frequency))\n",
        "    self.inverse_document_frequency_index = index;\n",
        "\n",
        "  # compute the tfidf score for all words in a document \n",
        "  def computeTFIDF(self, document):\n",
        "    index = {}\n",
        "     # this is O(|D|)where D is all words in the document\n",
        "    for word, frequency in document.term_frequency_index.items():\n",
        "        # tf / idf \n",
        "        index[word] = frequency * self.inverse_document_frequency_index[word]\n",
        "    return index\n",
        "\n",
        "  # store the tfidf values for all documents in the corpus\n",
        "  def createDataFrame (self):\n",
        "    data = []\n",
        "    rows = []\n",
        "    # this is O(N * |D|) where D is all words in the document and N is the number of documents in a corpus\n",
        "    for document in self.documents:\n",
        "      index = self.computeTFIDF(document)\n",
        "      rows.append(document._id)\n",
        "      data.append(index)\n",
        "    self.data_frame = pd.DataFrame(data=data, index=rows).fillna(0)\n",
        "    \n",
        "  # adds document to search engine\n",
        "  def index (self, document): \n",
        "    # add document to documents list\n",
        "    self.documents.append(document)\n",
        "    # compute new idf (optimize later)\n",
        "    self.computeIDF()\n",
        "    # create data frame\n",
        "    self.createDataFrame()\n",
        "\n",
        "  # look up the tfidf score\n",
        "  def lookup_tfidf (self, keyword):\n",
        "    # return empty list if keyword doesn't exist in any document\n",
        "    if keyword not in self.data_frame.columns:\n",
        "      return []\n",
        "    # get the rows (documents) in which the keyword exists\n",
        "    rows = self.data_frame.index[self.data_frame[keyword] > 0].tolist()\n",
        "    data = []\n",
        "    # look up the scores for each row \n",
        "    for row in rows:\n",
        "      score = self.data_frame.iloc[row][keyword]\n",
        "      data.append({\"id\": row, \"document\": self.documents[row].document, \"score\": score})\n",
        "    return data\n",
        "    \n",
        "  # combine duplictes/sum scores, and order according to relevancy \n",
        "  def process_results (self, lookup_list):\n",
        "    results = {}\n",
        "    # combine duplicates\n",
        "    for item in lookup_list:\n",
        "      if str(item[\"id\"]) not in results:\n",
        "        results[str(item[\"id\"])] = item \n",
        "      else:\n",
        "        results[str(item[\"id\"])][\"score\"] += item[\"score\"]\n",
        "    results = results.values()\n",
        "    # order in descending order \n",
        "    return sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
        "         \n",
        "\n",
        "  # search! \n",
        "  def search (self, keyword):\n",
        "    # process keyword into tokens \n",
        "    search_tokens = tokenize(keyword)\n",
        "    # query the data frame\n",
        "    results = []\n",
        "    for token in search_tokens:\n",
        "     lookup_list = self.lookup_tfidf(token)\n",
        "     for item in lookup_list:\n",
        "       results.append(item)\n",
        "    return self.process_results(results)\n",
        "  \n",
        "\n",
        "  def keywords (self, _id, n):\n",
        "    data = self.data_frame.loc[_id] # get tf_idf data by label\n",
        "    sorted = data.sort_values(ascending=False)\n",
        "    keys = sorted.keys()\n",
        "    return keys[:n]\n",
        "\n",
        "\n",
        "# Big of Indexing: O(N * |D|) + O(N * |D|) + O(T)\n",
        "# = O(N * |D|) where N = number of documents, and D is words in a document\n",
        "\n",
        "# Lucene uses optimized data structures to implement indexing \n",
        "# https://stackoverflow.com/questions/2602253/how-does-lucene-index-documents\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}